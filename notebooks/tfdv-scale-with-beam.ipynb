{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "compound-record",
   "metadata": {},
   "source": [
    "# TensorFlow Data Validation (TFDV): Scaling with Apache Beam\n",
    "\n",
    "This notebook demonstrates how to use TensorFlow Data Validation (TFDV) with Apache Beam and DataFlow. It reuses the introduction to TFDV made in the previous lab.\n",
    "\n",
    "- Extract data from BigQuery to GCS using DataFlow.\n",
    "- Compute the summary statistics using TFDV and DataFlow.\n",
    "- Explore the statistics to understand information about the data.\n",
    "- Infer the schema.\n",
    "- Save the updated schema to be used as a contract during inference.\n",
    "\n",
    "### Dataset\n",
    "\n",
    "This notebook uses [Chicago crime data](https://data.cityofchicago.org/) published as a public dataset in BigQuery. This dataset reflects reported incidents of crime that occurred in the City of Chicago from 2001 to present, minus the most recent seven days. The data will be extracted with the following columns:\n",
    "\n",
    "- **date**: Date when the incident occurred. this is sometimes a best estimate.\n",
    "- **iucr**: The Illinois Unifrom Crime Reporting code.\n",
    "- **primary_type**: The primary description of the IUCR code.\n",
    "- **location_description**: Description of the location where the incident occurred.\n",
    "- **arrest**: Indicates whether an arrest was made.\n",
    "- **domestic**: Indicates whether the incident was domestic-related as defined by the Illinois Domestic Violence Act.\n",
    "- **district**: Indicates the police district where the incident occurred. \n",
    "- **ward**: The ward (City Council district) where the incident occurred.\n",
    "- **fbi_code**: Indicates the crime classification.\n",
    "- **year**: Year the incident occurred.\n",
    "\n",
    "### Installing dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "associate-diamond",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow tensorflow_data_validation google-cloud-bigquery"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alpha-contract",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "czech-ownership",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "from google.cloud import bigquery\n",
    "import tensorflow_data_validation as tfdv\n",
    "import pandas as pd\n",
    "from tensorflow.python.lib.io import file_io\n",
    "from google.protobuf import text_format\n",
    "import apache_beam as beam \n",
    "from datetime import datetime\n",
    "\n",
    "GCS_BUCKET = \"\" # Set your GCS bucket\n",
    "PROJECT_ID = '' # Set your GCP Project Id\n",
    "REGION = '' # Set the region for Dataflow jobs\n",
    "LOCAL = False\n",
    "\n",
    "CHICAGO_CRIME_TABLE = 'bigquery-public-data.chicago_crime.crime'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "completed-interstate",
   "metadata": {},
   "source": [
    "## Extract data from BigQuery\n",
    "\n",
    "Instead of extracting data from 2019 (260,673 records), let's extract data between 2015 and 2019 (1,331,957 records). We will use the same SQL query, but this time we will use DataFlow. We will convert the records into TFRecord format as Tensorflow usually performs better on this type of file, but you could also use CSV if you want to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "apparent-smooth",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_query(year_from= None, year_to= None, limit= None) -> str:\n",
    "    query = f\"\"\"\n",
    "        SELECT \n",
    "            FORMAT_DATE('%Y',  CAST(date AS DATE)) AS crime_year,\n",
    "            FORMAT_DATE('%b',  CAST(date AS DATE)) AS crime_month,\n",
    "            FORMAT_DATE('%d',  CAST(date AS DATE)) AS crime_day, \n",
    "            FORMAT_DATE('%a',  CAST(date AS DATE)) AS crime_day_of_week, \n",
    "            iucr,\n",
    "            primary_type,\n",
    "            location_description,\n",
    "            CAST(domestic AS INT64) AS domestic,\n",
    "            district,\n",
    "            ward,\n",
    "            fbi_code,\n",
    "            CAST(arrest AS INT64) AS arrest,\n",
    "        FROM \n",
    "          {CHICAGO_CRIME_TABLE}\n",
    "        \"\"\"\n",
    "    if year_from:\n",
    "        query += f\"WHERE year >= {year_from}\"\n",
    "        if year_to:\n",
    "            query += f\" AND year <= {year_to} \\n\"\n",
    "    if limit:\n",
    "        query  += f\"LIMIT {limit}\"\n",
    "        \n",
    "    return query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "black-announcement",
   "metadata": {},
   "source": [
    "## Dataflow Pipeline\n",
    "\n",
    "To convert to TFRecord, we need a function that returns each row of our dataset into a tf.Example record. The function below will help us to do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "minus-final",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TYPES = {\n",
    "    'crime_year': 'STRING',\n",
    "    'crime_month': 'STRING',\n",
    "    'crime_day': 'STRING',\n",
    "    'crime_day_of_week': 'STRING',\n",
    "    'iucr': 'STRING',\n",
    "    'primary_type': 'STRING',\n",
    "    'location_description': 'STRING',\n",
    "    'fbi_code': 'STRING',\n",
    "    'domestic': 'INTEGER',\n",
    "    'district': 'INTEGER',\n",
    "    'ward': 'INTEGER',\n",
    "    'arrest': 'INTEGER'\n",
    "}\n",
    "\n",
    "def to_example(row, type_mapping):\n",
    "    features = {}\n",
    "    for name, value in row.items():\n",
    "        datatype = type_mapping[name]\n",
    "        if value is None:\n",
    "            features[name] = tf.train.Feature()\n",
    "        elif datatype == 'INTEGER':\n",
    "            features[name] = tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
    "        else:\n",
    "            features[name] = tf.train.Feature(bytes_list=tf.train.BytesList(value=[tf.compat.as_bytes(value)]))\n",
    "    return tf.train.Example(features=tf.train.Features(feature=features))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assisted-content",
   "metadata": {},
   "source": [
    "Now, let's define our DataFlow pipeline. There are 4 steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "relative-cartridge",
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_pipeline(query, output_location, runner, type_mapping, args):\n",
    "    options = beam.options.pipeline_options.GoogleCloudOptions(**args)\n",
    "    \n",
    "    with beam.Pipeline(runner, options=options) as pipeline:\n",
    "        (pipeline \n",
    "             | \"Extract dataset\">> beam.io.Read(beam.io.ReadFromBigQuery(query=query, use_standard_sql=True))\n",
    "             | \"Convert\" >> beam.Map(lambda instance: to_example(instance, type_mapping))\n",
    "             | \"Serialize\" >> beam.Map(lambda example: example.SerializeToString(deterministic=True))\n",
    "             | \"Save as TFRecords\" >> beam.io.WriteToTFRecord(file_path_prefix = output_location, file_name_suffix=\".tfrecords\")\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "geological-cyprus",
   "metadata": {},
   "source": [
    "We also define a setup.py to install tfdv on the workers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "final-breed",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile setup.py\n",
    "\n",
    "from setuptools import setup\n",
    "\n",
    "setup(\n",
    "    name='tfdv',\n",
    "    install_requires=[\n",
    "      'tensorflow_data_validation'\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "technological-lesbian",
   "metadata": {},
   "source": [
    "Let's define some constraints for our job, and let's trigger it. It will take about 15 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "medieval-magnet",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_time = datetime.now().strftime('%Y%m%d-%H%M%S')\n",
    "root_folder = f\"gs://{GCS_BUCKET}/tfdv/{job_time}\" if not LOCAL else '.'\n",
    "runner = \"DataflowRunner\" if not LOCAL else \"DirectRunner\"\n",
    "job_name = f\"tfdv-chicago-crime-{job_time}\"\n",
    "year_from =  2015\n",
    "year_to = 2019\n",
    "data_folder = root_folder+\"/data/train/\"\n",
    "query = generate_query(year_from, year_to, limit=None if not LOCAL else 10000)\n",
    "args = {\n",
    "    'job_name': job_name,\n",
    "    'runner': runner,\n",
    "    'project': PROJECT_ID,\n",
    "    'region': REGION,\n",
    "    'save_main_session': True,\n",
    "    'staging_location': root_folder+\"/staging/\",\n",
    "    'temp_location': root_folder+\"/temp/\",\n",
    "    'setup_file': './setup.py'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proved-sleeping",
   "metadata": {},
   "outputs": [],
   "source": [
    "execute_pipeline(query, data_folder, runner, DATA_TYPES, args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suffering-carnival",
   "metadata": {},
   "source": [
    "Once the job completes, confirm the data has been correctly extracted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "manufactured-universe",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil ls {data_folder}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cubic-pakistan",
   "metadata": {},
   "source": [
    "## Generating statistics\n",
    "\n",
    "The data is bigger than in the previous notebook. To generate the statistics, we still need to use Apache Beam and DataFlow, so we specify the runtime to use.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mature-today",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_name = f\"tfdv-chicago-crime-stats-{job_time}\"\n",
    "args['job_name'] = job_name\n",
    "stats_location = root_folder + \"/stats/stats.pb\"\n",
    "options =  beam.options.pipeline_options.GoogleCloudOptions(**args)\n",
    "\n",
    "_ = tfdv.generate_statistics_from_tfrecord(\n",
    "    data_location=data_folder, \n",
    "    output_path=stats_location,\n",
    "    stats_options=tfdv.StatsOptions(\n",
    "        sample_rate=.3\n",
    "    ),\n",
    "    pipeline_options = options\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sitting-effort",
   "metadata": {},
   "source": [
    "Confirm we have correctly computed the statistics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fixed-facial",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil ls {stats_location}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "center-continent",
   "metadata": {},
   "source": [
    "## Load statistics and infer schema\n",
    "\n",
    "The next step is to read the statistics from GCS. Then, we can explore, infer the schema, and update constraints as we did in the previous lab. This does not require Apache Beam as we are just working with the statistics. As this is very similar to the previous lab, I don't include all the steps here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "federal-world",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = tfdv.load_statistics(stats_location)\n",
    "tfdv.visualize_statistics(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "clinical-explosion",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = tfdv.infer_schema(statistics=stats)\n",
    "tfdv.display_schema(schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "governing-nebraska",
   "metadata": {},
   "source": [
    "## End of lab\n",
    "\n",
    "The lab ends here as the next steps are the same as previously. You could try to reuse the Beam pipeline to extract 2020 data and compute the statistics. In the next lab, we will see how you can include this schema validation as part of your ML pipeline using Tensorflow Pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "painted-found",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
