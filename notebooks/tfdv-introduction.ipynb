{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "vanilla-russell",
   "metadata": {},
   "source": [
    "# Introduction to TensorFlow Data Validation (TFDV)\n",
    "\n",
    "This notebook demonstrates how to use TensorFlow Data Validation (TFDV) to analyze and validate structured data. In addition to testing code, an ML pipeline must also test data and look for anomalies, compare training and evaluation datasets and make sure they are consistent. TFDV is a tool that can help to generateÂ descriptive statistics, inferring schema, and detecting drift and skew.\n",
    "\n",
    "This lab shows you how to use TFDV during the data exploratory phase of your model deployment. The goal is to:\n",
    "\n",
    "- Extract data from BigQuery.\n",
    "- Compute the summary statistics.\n",
    "- Explore the computed statistics to understand information about the data.\n",
    "- Infer an initial schema.\n",
    "- Validate and update the schema based on a new dataset from BigQuery.\n",
    "- Save the updated schema to be used as a contract during inference.\n",
    "\n",
    "### Dataset\n",
    "\n",
    "This notebook uses [Chicago crime data](https://data.cityofchicago.org/) published as a public dataset in BigQuery. This dataset reflects reported incidents of crime that occurred in the City of Chicago from 2001 to present, minus the most recent seven days. The data will be extracted with the following columns:\n",
    "\n",
    "- **date**: Date when the incident occurred. this is sometimes a best estimate.\n",
    "- **iucr**: The Illinois Unifrom Crime Reporting code.\n",
    "- **primary_type**: The primary description of the IUCR code.\n",
    "- **location_description**: Description of the location where the incident occurred.\n",
    "- **arrest**: Indicates whether an arrest was made.\n",
    "- **domestic**: Indicates whether the incident was domestic-related as defined by the Illinois Domestic Violence Act.\n",
    "- **district**: Indicates the police district where the incident occurred. \n",
    "- **ward**: The ward (City Council district) where the incident occurred.\n",
    "- **fbi_code**: Indicates the crime classification.\n",
    "- **year**: Year the incident occurred.\n",
    "\n",
    "### Installing dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "widespread-ghana",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow tensorflow_data_validation google-cloud-bigquery"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "creative-sellers",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "polished-oakland",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "import tensorflow_data_validation as tfdv\n",
    "import pandas as pd\n",
    "from tensorflow.python.lib.io import file_io\n",
    "from google.protobuf import text_format\n",
    "from tensorflow_metadata.proto.v0 import schema_pb2\n",
    "\n",
    "CHICAGO_CRIME_TABLE = 'bigquery-public-data.chicago_crime.crime'\n",
    "bq_client = bigquery.Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "analyzed-creature",
   "metadata": {},
   "source": [
    "## Extract data from BigQuery\n",
    "\n",
    "Our dataset is public in BigQuery. If not done yet, ensure your environment is correctly set up to access GCP (export GOOGLE_APPLICATION_CREDENTIALS). First, let's get 5 records to confirm we can query the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "certain-folder",
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_query(client: bigquery.Client, query: str) -> pd.DataFrame:\n",
    "    query_job = bq_client.query(query)\n",
    "    results = query_job.result()\n",
    "    return results.to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "north-african",
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPLORATION_QUERY = f\"\"\"\n",
    "    SELECT\n",
    "        date,\n",
    "        iucr,\n",
    "        primary_type,\n",
    "        location_description,\n",
    "        arrest,\n",
    "        domestic,\n",
    "        district,\n",
    "        ward,\n",
    "        fbi_code\n",
    "    FROM\n",
    "      {CHICAGO_CRIME_TABLE}\n",
    "    LIMIT 5\n",
    "\"\"\"\n",
    "results = execute_query(bq_client, EXPLORATION_QUERY)\n",
    "results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "velvet-fellow",
   "metadata": {},
   "source": [
    "Feel free to explore the dataset if you want to, as not all attributes have been included. In the next sections, we will use crime data from 2019 to generate the statistics and a reference schema, then we will validate 2020 data against it.\n",
    "\n",
    "Now, let's extract data from 2019."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "current-raise",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_query(year_from: int = None, year_to: int = None, limit: int = None) -> str:\n",
    "    query = f\"\"\"\n",
    "        SELECT \n",
    "            FORMAT_DATE('%Y',  CAST(date AS DATE)) AS crime_year,\n",
    "            FORMAT_DATE('%b',  CAST(date AS DATE)) AS crime_month,\n",
    "            FORMAT_DATE('%d',  CAST(date AS DATE)) AS crime_day, \n",
    "            FORMAT_DATE('%a',  CAST(date AS DATE)) AS crime_day_of_week, \n",
    "            iucr,\n",
    "            primary_type,\n",
    "            location_description,\n",
    "            CAST(domestic AS INT64) AS domestic,\n",
    "            district,\n",
    "            ward,\n",
    "            fbi_code,\n",
    "            CAST(arrest AS INT64) AS arrest,\n",
    "        FROM \n",
    "          {CHICAGO_CRIME_TABLE}\n",
    "        \"\"\"\n",
    "    if year_from:\n",
    "        query += f\"WHERE year >= {year_from}\"\n",
    "        if year_to:\n",
    "            query += f\" AND year <= {year_to} \\n\"\n",
    "    if limit:\n",
    "        query  += f\"LIMIT {limit}\"\n",
    "        \n",
    "    return query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "secure-numbers",
   "metadata": {},
   "outputs": [],
   "source": [
    "crime_df = execute_query(bq_client, generate_query(2019, 2019))\n",
    "crime_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "guilty-ordinance",
   "metadata": {},
   "source": [
    "## Compute summary statistics\n",
    "\n",
    "If we want to use this data to build a model, we need to generate baseline statistics that we can use to compare with more recent data and ensure there is no skew or drift. Currently, our data is in a pandas dataframe, so we can use [tfdv.generate_statistics_from_dataframe](https://www.tensorflow.org/tfx/data_validation/api_docs/python/tfdv/generate_statistics_from_dataframe) to generate the statistics. Similar functions exist to compute statistics from TF Records and CSV datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "metallic-northern",
   "metadata": {},
   "outputs": [],
   "source": [
    "crime_2019_stats = tfdv.generate_statistics_from_dataframe(crime_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "electronic-equilibrium",
   "metadata": {},
   "source": [
    "We can visualize the statistics using [tfdv.visualize_statistics](https://www.tensorflow.org/tfx/data_validation/api_docs/python/tfdv/visualize_statistics). It uses Facets to create a succinct visualization of our data and helps to identify common bugs like unbalanced datasets. Feel free to explore the filters and other features this tool offers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "twelve-fabric",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfdv.visualize_statistics(crime_2019_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "driven-series",
   "metadata": {},
   "source": [
    "Using Facets, you can quickly and easily spot issues, identify data ranges, categorical attribute values, etc. For example, you could use \"Sort by missing/zeroes\" to quickly identify attributes with a lot of null or 0 values, and decide if it's expected or if something needs to be fixed in your data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inner-coaching",
   "metadata": {},
   "source": [
    "## Generate Schema\n",
    "\n",
    "After deploying your pipeline to production, you may not be aware of changes in the data source. For example, an attribute used by your model could be dropped by the source system, or the data type could be converted from integer to string. If you don't detect these changes, the downstream steps of your pipeline may not succeed, or the performance of your model may decrease. Generating a schema and ensuring all new datasets going through your ML pipeline follow the same structure make your solution more robust and reliable.\n",
    "\n",
    "\n",
    "Using the statistics that we have generated earlier, let's infer the schema using [tfdv.infer_schema](https://www.tensorflow.org/tfx/data_validation/api_docs/python/tfdv/infer_schema) and [display_schema](https://www.tensorflow.org/tfx/data_validation/api_docs/python/tfdv/display_schema)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accessory-textbook",
   "metadata": {},
   "outputs": [],
   "source": [
    "crime_2019_schema = tfdv.infer_schema(statistics=crime_2019_stats)\n",
    "tfdv.display_schema(schema=crime_2019_schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blank-transport",
   "metadata": {},
   "source": [
    "This schema is inferred, meaning that it can be enhanced. This is strongly encouraged if you want to be able to detect data skew and data drift."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ranging-dominican",
   "metadata": {},
   "source": [
    "## Updating initial schema\n",
    "\n",
    "The feature `domestic` has been converted from a boolean to an integer, so the values should be 0 or 1. We also know districts in Chicago can be between 1 and 31, so we can set our domain accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sophisticated-morocco",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfdv.set_domain(crime_2019_schema, \"domestic\", schema_pb2.IntDomain(min=0, max=1))\n",
    "tfdv.set_domain(crime_2019_schema, \"district\", schema_pb2.IntDomain(min=1, max=31))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "activated-retail",
   "metadata": {},
   "source": [
    "If you display the new schema, you should see the domain has been updated as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "perceived-airline",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfdv.display_schema(schema=crime_2019_schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "racial-round",
   "metadata": {},
   "source": [
    "## Validating schema\n",
    "\n",
    "We have used data from 2019 to generate the schema. Let's try to validate it with 2020 data. We extract the data from BigQuery, and generate the statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acoustic-pittsburgh",
   "metadata": {},
   "outputs": [],
   "source": [
    "crime_2020_df = execute_query(bq_client, generate_query(2020, 2020))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "painful-morrison",
   "metadata": {},
   "outputs": [],
   "source": [
    "crime_2020_stats = tfdv.generate_statistics_from_dataframe(crime_2020_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inappropriate-tennessee",
   "metadata": {},
   "source": [
    "First, let's see how you can visually compare the statistics using tfdv.visualize_statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "normal-clerk",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfdv.visualize_statistics(\n",
    "    lhs_statistics=crime_2019_stats,\n",
    "    rhs_statistics=crime_2020_stats,\n",
    "    lhs_name='2019',\n",
    "    rhs_name='2020'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eastern-running",
   "metadata": {},
   "source": [
    "This is an easy way to compare the values. You can quickly see the total number of crimes in 2020 is lower than in 2019, but the percentage of cases where an arrest has been made is also lower."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handy-relaxation",
   "metadata": {},
   "source": [
    "Let's do a programmatic comparison now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alien-federation",
   "metadata": {},
   "outputs": [],
   "source": [
    "anomalies = tfdv.validate_statistics(\n",
    "    statistics=crime_2020_stats, \n",
    "    schema=crime_2019_schema,\n",
    "    previous_statistics=crime_2019_stats\n",
    ")\n",
    "tfdv.display_anomalies(anomalies)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dirty-prospect",
   "metadata": {},
   "source": [
    "We can see one anomaly being detected. The feature `primary_type` is a categorical feature, and there is a new value that wasn't in the original dataset in 2019. This error shouldn't be flagged as an error, because based on our business knowledge we know RITUALISM is a valid `primary_type`. So let's update our schema and the domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sonic-tomato",
   "metadata": {},
   "outputs": [],
   "source": [
    "primary_types = tfdv.get_domain(crime_2019_schema, 'primary_type')\n",
    "primary_types.value.append('RITUALISM')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worth-healing",
   "metadata": {},
   "source": [
    "Let's recompute the anomalies, see if it has fixed the problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sustainable-update",
   "metadata": {},
   "outputs": [],
   "source": [
    "anomalies = tfdv.validate_statistics(\n",
    "    statistics=crime_2020_stats, \n",
    "    schema=crime_2019_schema,\n",
    "    previous_statistics=crime_2019_stats\n",
    ")\n",
    "tfdv.display_anomalies(anomalies)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "still-latter",
   "metadata": {},
   "source": [
    "Looks good! There are many more ways to update your schema and apply more constraints, especially for detecting skew and drift. Have a look at [the list of anomalies](https://www.tensorflow.org/tfx/data_validation/anomalies) that can be identified by tfdv.\n",
    "\n",
    "### Additional constraint examples\n",
    "\n",
    "We can see `location_description` has 0.57% missing values in 2020 vs 0.45% in 2019. Let's say you want to set a threshold of 0.5% of missing values max. You could do it like below: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "serious-things",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfdv.get_feature(crime_2019_schema, 'location_description').presence.min_fraction = 0.995\n",
    "anomalies = tfdv.validate_statistics(\n",
    "    statistics=crime_2020_stats, \n",
    "    schema=crime_2019_schema,\n",
    "    previous_statistics=crime_2019_stats\n",
    ")\n",
    "tfdv.display_anomalies(anomalies)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "modular-beginning",
   "metadata": {},
   "source": [
    "Let's also try to add drift detection. TFDV uses [L-infinity norm](https://en.wikipedia.org/wiki/L-infinity) to identify drifts, so we just need to set the maximum threshold we are ready to accept. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expressed-installation",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfdv.get_feature(crime_2019_schema, 'primary_type').drift_comparator.infinity_norm.threshold = 0.01\n",
    "anomalies = tfdv.validate_statistics(\n",
    "    statistics=crime_2020_stats, \n",
    "    schema=crime_2019_schema,\n",
    "    previous_statistics=crime_2019_stats\n",
    ")\n",
    "tfdv.display_anomalies(anomalies)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conscious-square",
   "metadata": {},
   "source": [
    "In this example, we can see there is a drift for the \"THEFT\" type of crime between 2019 and 2020.\n",
    "\n",
    "Let's remove these two constraints for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "empty-problem",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfdv.get_feature(crime_2019_schema, 'primary_type').drift_comparator.infinity_norm.threshold = 1.0\n",
    "tfdv.get_feature(crime_2019_schema, 'location_description').presence.min_fraction = 0.0\n",
    "anomalies = tfdv.validate_statistics(\n",
    "    statistics=crime_2020_stats, \n",
    "    schema=crime_2019_schema,\n",
    "    previous_statistics=crime_2019_stats\n",
    ")\n",
    "tfdv.display_anomalies(anomalies)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "italic-saver",
   "metadata": {},
   "source": [
    "## Handling different environments\n",
    "\n",
    "If you want to use this dataset to predict if an arrest will be made or not, you will have the flag `arrest` during training, but not during serving time. The schema needs to be updated to be aware of this difference depending on the environment.\n",
    "\n",
    "For example, let's say 2020 data is our serving dataset. Let's drop the `arrest` attribute, and check for anomalies. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "military-cliff",
   "metadata": {},
   "outputs": [],
   "source": [
    "crime_serving_df = crime_2020_df.drop([\"arrest\"], axis=1)\n",
    "crime_serving_stats = tfdv.generate_statistics_from_dataframe(crime_serving_df)\n",
    "anomalies = tfdv.validate_statistics(\n",
    "    statistics=crime_serving_stats, \n",
    "    schema=crime_2019_schema,\n",
    "    previous_statistics=crime_2019_stats\n",
    ")\n",
    "tfdv.display_anomalies(anomalies)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adequate-replication",
   "metadata": {},
   "source": [
    "As expected, we see a feature is missing. Let's indicate this is expected for serving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "confidential-digest",
   "metadata": {},
   "outputs": [],
   "source": [
    "crime_2019_schema.default_environment.append('TRAINING')\n",
    "crime_2019_schema.default_environment.append('SERVING')\n",
    "tfdv.get_feature(crime_2019_schema, 'arrest').not_in_environment.append('SERVING')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vital-causing",
   "metadata": {},
   "outputs": [],
   "source": [
    "anomalies = tfdv.validate_statistics(crime_serving_stats, crime_2019_schema, environment='SERVING')\n",
    "tfdv.display_anomalies(anomalies)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "competitive-jacob",
   "metadata": {},
   "source": [
    "And now it's ok!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bulgarian-utilization",
   "metadata": {},
   "source": [
    "## Saving your schema\n",
    "\n",
    "Once you are happy with your schema, you can save it so that you can reuse it in your pipeline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "silver-grill",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_file = 'schema.pbtxt'\n",
    "tfdv.write_schema_text(crime_2019_schema, schema_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "twenty-vienna",
   "metadata": {},
   "source": [
    "## End of lab\n",
    "\n",
    "In this lab, we have seen how to generate statistics from a dataset, and how to visually explore them using Facets. We have seen how to generate and update a schema, and then how to apply it to identify anomalies in the data. \n",
    "\n",
    "There are other TFDV features that we haven't covered, for example, how to slice the data by a specific feature before extracting the statistics. You can check out the official documentation for more details on this topic.\n",
    "\n",
    "We have used data from 2019 to generate the initial schema, but if your dataset is bigger, you may need to execute this code using Cloud computing. TFDV has an Apache Beam runtime, so in the next lab, we will see how you could do the same steps using DataFlow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "joint-digest",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
